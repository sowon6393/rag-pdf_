import streamlit as st
from langchain_core.messages import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.document_loaders import PyPDFLoader
import os

# OPENAI API KEY ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
st.title("Sâš¬Wâš¬Nâš¬Bâš¬T ğŸ‘½")
st.subheader("ë¬¼ì–´ë´ ê·¼ë° ë§ì´ ë¬¼ì–´ë³´ë©´ ìª¼ë” ê³¤ë€í•¨ ğŸ’¸")
st.markdown(":green-background[ ğŸ‘ˆ PDF íŒŒì¼ ì—…ë¡œë“œ ì‹œ PDFë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤. ]")
st.markdown(':blue-background[ í™ë„ë¹„ ë°”ë³´ ğŸ¤ªğŸ¤¸]')

retrieval_chain = None

import streamlit as st

# st.page_link("main.py", label="Home", icon="ğŸ ")
# st.page_link("pages/page_1.py", label="Page 1", icon="1ï¸âƒ£")
# st.page_link("pages/page_2.py", label="Page 2", icon="2ï¸âƒ£", disabled=True)
# st.page_link("http://www.google.com", label="Google", icon="ğŸŒ")

with st.sidebar:

    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒˆì„±
    button = st.button("ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”")
    # íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•œ ìœ„ì ¯ ìƒì„±
    uploaded_file = st.file_uploader("íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”", type=["pdf"])

    # api_key = st.text_input(
    #     "API KEY",
    #     type="password",
    # )
    api_key = st.secrets["OPENAI_API_KEY"]

    # í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•  ìˆ˜ ìˆëŠ” ì˜µì…˜ì„ ì£¼ê² ë‹¤!
    # option = st.selectbox(
    #     "í”„ë¡¬í”„íŠ¸",
    #     PROMPT_LIST,
    #     index=0,
    #     placeholder="í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”",
    # )

    system_prompt = st.text_area(
        "ë‚˜ëŠ”ì•¼ ì†Œì›ë´‡ ğŸš€ ",
        "ë‹¹ì‹ ì˜ ì´ë¦„ì€ 'ì†Œì›ë´‡'ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” Assistantì…ë‹ˆë‹¤. ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.",
    )
    option = f"{system_prompt}" + "\n\n#Question: {question}"


# ë©”ì‹œì§€ ê¸°ë¡ì„ ìœ„í•œ ì €ì¥ì†Œë¥¼ ìƒì„±
if "messages" not in st.session_state:
    st.session_state["messages"] = []

if button:
    # ë©”ì‹œì§€ ê¸°ë¡ì„ ì´ˆê¸°í™”
    st.session_state["messages"] = []

if not os.path.exists(".cache"):
    os.mkdir(".cache")


# íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ëŠ” í•¨ìˆ˜
def upload_file(file):
    file_content = file.read()
    file_path = f"./.cache/{file.name}"

    with open(file_path, "wb") as f:
        f.write(file_content)

    # Step 1: ë¬¸ì„œ ë¡œë“œ
    # loader = TextLoader("data/appendix-keywords.txt")
    # PDF íŒŒì¼ ë¡œë“œ. íŒŒì¼ì˜ ê²½ë¡œ ì…ë ¥
    loader = PyPDFLoader(file_path)

    # Step 2: ë¬¸ì„œ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # 500ìë¡œ ë¬¸ì„œë¥¼ ë¶„í• 
        chunk_overlap=50,  # 50ìì˜ ì¤‘ë³µì„ í—ˆìš©
        length_function=len,
    )
    docs = loader.load_and_split(text_splitter)

    # Step 3: ë²¡í„° ì €ì¥ì†Œ ìƒì„± & ì„ë² ë”©(ë¬¸ì¥ì„ ìˆ«ì í‘œí˜„ìœ¼ë¡œ ë°”ê¾¼ë‹¤!!) -> ì €ì¥
    vectorstore = FAISS.from_documents(docs, OpenAIEmbeddings())

    # Step 4: ê²€ìƒ‰ê¸°(retriever) -> ë‚˜ì¤‘ì— ì§ˆë¬¸(Query) ì— ëŒ€í•œ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ í•˜ê¸° ìœ„í•¨
    retriever = vectorstore.as_retriever()
    return retriever


# ë§Œì•½ì— íŒŒì¼ì´ ì—…ë¡œë“œê°€ ëœë‹¤ë©´...
if uploaded_file:
    retriever = upload_file(uploaded_file)

    # Step 5: í”„ë¡¬í”„íŠ¸ ì‘ì„±, context: ê²€ìƒ‰ê¸°ì—ì„œ ê°€ì ¸ì˜¨ ë¬¸ì¥, question: ì§ˆë¬¸
    template = """ë‹¹ì‹ ì€ ë¬¸ì„œì— ëŒ€í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì¹œì ˆí•œ Assistant ì…ë‹ˆë‹¤. ë¬´ì¡°ê±´, ì£¼ì–´ì§„ Context ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
    ë‹µë³€ì— ëŒ€í•œ ì¶œì²˜ë„ í•¨ê»˜ ì œê³µí•´ ì£¼ì„¸ìš”.
    ì¶œì²˜ëŠ” íŒŒì¼ ì´ë¦„ê³¼ í˜ì´ì§€ ë²ˆí˜¸ë¡œ í‘œê¸°í•´ ì£¼ì„¸ìš”.
    
    #Context:
    {context}

    #Question:
    {question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    # Step 6: OpenAI GPT-4 ëª¨ë¸ì„ ì„¤ì •
    model = ChatOpenAI(model="gpt-4o", streaming=True, api_key=api_key)

    # Step 7: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ê¸° ìœ„í•œ ì²´ì¸ ìƒì„±ë‹¤
    retrieval_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | model
        | StrOutputParser()
    )


# ë©”ì‹œì§€ë¥¼ ì¶”ê°€ë§Œ í•´ì£¼ëŠ” í•¨ìˆ˜
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


# ì´ì „ ëŒ€í™”ê¸°ë¡ì„ ì „ì²´ë¥¼ ì¶œë ¥í•´ì£¼ëŠ” í•¨ìˆ˜
def print_chat_messages():
    for message in st.session_state["messages"]:
        st.chat_message(message.role).write(message.content)


# ì¶œë ¥í•´ì£¼ëŠ” í•¨ìˆ˜ë¥¼ í˜¸ì¶œ
print_chat_messages()

user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

if user_input:
    st.chat_message("user").write(user_input)
    # ChatGPT ì—ê²Œ ì§ˆë¬¸
    with st.chat_message("assistant"):
        response_container = st.empty()
        if retrieval_chain is not None:
            chunk = []

            answer = retrieval_chain.stream(user_input)
            tmp_answer = "<PDF ë¬¸ì„œì— ê¸°ë°˜í•œ ë‹µë³€í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤>\n\n"
            for c in answer:
                tmp_answer += c
                response_container.markdown(tmp_answer)
                chunk.append(c)

            response = "".join(chunk)
        else:
            # ì²´ì¸ì„ ìƒì„±
            prompt = PromptTemplate.from_template(option)

            model = ChatOpenAI(model="gpt-4o", temperature=0, api_key=api_key)

            chain = prompt | model | StrOutputParser()

            esponse_container = st.empty()
            chunk = []

            answer = chain.stream({"question": user_input})
            tmp_answer = ""
            for c in answer:
                tmp_answer += c
                response_container.markdown(tmp_answer)
                chunk.append(c)

            response = "".join(chunk)

    # ë©”ì‹œì§€ ê¸°ë¡ì„ ìœ„í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ìºì‹±(ì €ì¥ì†Œ)ì— ì €ì¥í•©ë‹ˆë‹¤.
    add_message("user", user_input)
    add_message("assistant", response)
